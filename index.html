<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<!--<link rel="shortcut icon" href="./files/icon.ico">-->
<title>Shigang Li (李士刚), SPCL, ETH Zurich - Homepage</title>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 


<class="staffshortcut">
 </font><font size="5"> 
 <A HREF="#Selected Publications">Publications</A> |
 <A HREF="#Projects">Projects</A> |
 <A HREF="#Teaching">Teaching</A> |
 <A HREF="#Talks">Talks</A> |
 <A HREF="#Services">Services</A> |
 <A HREF="#News">News</A>
 </font>
<br />
<br />

<table class="imgtable"><tr><td>
<a href="./"><img src="./files/photo.png" alt="" height="190px" /></a>&nbsp;</td>
<td align="left"><p><font size="5"><b>Shigang Li (</font><font size="5"; font style="font-family:Sun">李士刚</font>)</b><br />

</font><font size="4">
	<b><font color="#002366"> Postdoctoral Researcher</font></b>, <a href="https://spcl.inf.ethz.ch/"><b><font color="#002366">SPCL Lab</b>, </a> <a href="https://inf.ethz.ch/"><b><font color="#002366"> ETH Zurich</b></a> <br />
<b>Links: &nbsp; &nbsp; </b>[<a href="https://scholar.google.com.hk/citations?user=KEUan7IAAAAJ&hl=en" target="_blank">Google Scholar</a>] &nbsp; [<a href="https://www.researchgate.net/profile/Shigang_Li2" target="_blank">ResearchGate</a>] &nbsp; [<a href="https://www.linkedin.com/in/shigang-li-87aa1458/" target="_blank">LinkedIn</a>] &nbsp; [<a href="https://github.com/Shigangli/" target="_blank">GitHub</a>] <br /> <br />
<b>Research interests: </b> &nbsp; Parallel Computing, &nbsp; High Performance Deep Learning Systems, &nbsp; GPU, &nbsp; MPI, &nbsp; Heterogeneous Computing <br />
<b>For any questions: &nbsp; </b> <a href="">shigangli.cs@gmail.com</a>; &nbsp; <a href="">shigang.li@inf.ethz.ch</a> <br />
</font>

</p>
</td></tr></table>


<A NAME="Brief Biography"><h2>Brief Biography</h2></A>
</font><font size="4">
&nbsp; &nbsp; &nbsp; &nbsp; Dr. Shigang Li is currently a Postdoctoral Researcher in <a href="https://spcl.inf.ethz.ch/">SPCL Lab, </a><a href="https://inf.ethz.ch/">ETH Zurich</a> since Aug. 2018. His research interests include parallel and distributed deep learning systems, high performance computing, and heterogeneous computing. He received the Bachelor's degree majored in Computer Science and the Ph.D degree majored in Computer Architecture from University of Science and Technology Beijing, in 2009 and 2014, respectively. He has been a joint Ph.D student in Department of Computer Science, <a href="https://illinois.edu/">University of Illinois at Urbana-Champaign</a> from Sep. 2011 to Sep. 2013. He has been an Assistant Professor in State Key Laboratory of Computer Architecture, <a href="http://english.ict.cas.cn/">Institute of Computing Technology, Chinese Academy of Sciences</a> from 2014 to 2018. He got the Best Paper Nominations in SC'21, PPoPP'20 and HPDC'13, and Outstanding Paper of MLSys'21. He has served as the PC members in top conferences (SC, PPoPP, IPDPS, IEEE Cluster, ICPP, etc.) and the invited reviewers in prestigious journals (IEEE TPDS, IEEE TSC, IEEE TBD, JPDC, etc.). He is the Associate Editor of Cluster Computing, and has been the Publications Chair of IISWC'20 and the Workshop Co-Chair of ICS'18. He is an Executive Committee Member of CCF TCHPC, and an Executive Committee Member of ACM SIGHPC China Chapter. He is a member of ACM and IEEE, and a senior member of CCF.
</font>
 

<A NAME="News"><h2>News</h2></A>
<ul>
</font><font size="4">
<li> <b> <font color="#FF0000">[June 15, 2022]</font> </b>, very glad that <font color="blue">TWO papers are accepted by</font> <a href= "https://sc22.supercomputing.org/">SC 2022</a>. A thrilling story is that one paper was submitted in the last 10 seconds before the deadline. I would like to say don't rush to submit until the paper is good enough, but 10 seconds is definitely not elegant :-) </li>
<li> <b> <font color="#FF0000">[June 10, 2022]</font> </b>, very glad to serve as Publicity Chair (Europe) of <a href= "https://ppopp23.sigplan.org/">PPoPP 2023</a>. See call for papers <a href= "https://ppopp23.sigplan.org/track/PPoPP-2023-papers#Call-for-Papers">here</a>.</li>
<li> <b> <font color="#FF0000">[April 4, 2022]</font> </b>, talk in <a href= "https://ppopp22.sigplan.org/">PPoPP'22</a> &#151; Near-Optimal Sparse Allreduce for Distributed Deep Learning. [<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Video</a>]</li>
<li> <b> <font color="#FF0000">[Nov. 16, 2021]</font> </b>, talk in <a href= "https://sc21.supercomputing.org/program/papers/">SC'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145#sec-supp">Video</a>]</li>
<li> <b> <font color="#FF0000">[Aug. 28, 2021]</font> </b>, talk in <a href= "https://www.emc2-ai.org/virtual-21">ECM2'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://www.youtube.com/watch?v=0ydk8u8ipmM">Video</a>]</li>
<li> <b> <font color="#FF0000">[Feb. 24, 2020]</font> </b>, talk in <a href= "https://ppopp20.sigplan.org/">PPoPP'20</a> &#151; Taming unbalanced training workloads in deep learning with partial collective operations. [<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Video</a>]</li>
<li> <b> <font color="#FF0000">[Oct. 26, 2016]</font> </b>, talk in <a href= "https://sites.google.com/a/lbl.gov/padal-workshop/previous-padals/padal16/schedule">PADAL'16</a> &#151; Cache-Oblivious MPI All-to-All Collectives. [<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Video</a>]</li>
<!--
<li> <b> <font color="#FF0000">[2020.10]</font> </b> I'm playing a very interesting newly discovered Jigsaw Puzzle game :-) It's super cool! </li>
-->
</font>
</ul>


<A NAME="Selected Publications"><h2>Selected Publications</h2></A>
<ul>
</font><font size="4">

<li>
<span><b><font color="#002366">[ICS’2022]</font></b> &nbsp; Oliver Rausch, Tal Ben-Nun, Nikoli Dryden, Andrei Ivanov, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. A Data-Centric Optimization Framework for Machine Learning. The 36th ACM International Conference on Supercomputing, 2022. [<a href= "https://arxiv.org/abs/2110.10802">Paper</a>][<a href= "https://github.com/spcl/daceml">Code</a>]
</span>
</li>

<li> <span><b><font color="#002366">[PPoPP'2022]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Near-Optimal Sparse Allreduce for Distributed Deep Learning. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. [<a href= "https://dl.acm.org/doi/10.1145/3503221.3508399">Paper</a>][<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Talk</a>][<a href= "./files/oktopk-slides-shigangli.pdf">Slides</a>][<a href= "https://github.com/Shigangli/Ok-Topk">Code</a>] 
</span></li>

<li>
<span><b><font color="#002366">[SC'2021]</font></b> &nbsp; Daniele De Sensi, Salvatore Di Girolamo, Saleh Ashkboos, <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Flare: Flexible In-Network Allreduce. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2021. [<a href= "https://dl.acm.org/doi/10.1145/3458817.3476178">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2021]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2021. (<font color="#FF0000">Best Paper Finalist</font>) [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145">Paper</a>][<a href= "https://www.youtube.com/watch?v=164XCpIZk6o&t=0s">Talk</a>][<a href= "./files/Chimera-SC21-shigangli-slides.pdf">Slides</a>][<a href= "https://github.com/Shigangli/Chimera">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[NeurIPS’2021]</font></b> &nbsp; Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, <b><font color="#002366">Shigang Li</font></b>, Dan Alistarh. Asynchronous Decentralized SGD with Quantized and Local Updates. In Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems, 2021. [<a href= "https://proceedings.neurips.cc/paper/2021/hash/362c99307cdc3f2d8b410652386a9dd1-Abstract.html">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[MLSys’2021]</font></b> &nbsp; Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. Data Movement is All You Need: A Case Study on Optimizing Transformers. The 4th Conference on Machine Learning and Systems, 2021. (<font color="#FF0000">Outstanding Paper</font>, 5/52) [<a href= "https://proceedings.mlsys.org/paper/2021/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf">Paper</a>][<a href= "https://github.com/spcl/substation">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[T SUSTAIN ENERG’2021]</font></b> &nbsp; Tiechui Yao, Jue Wang, Haoyan Wu, Pei Zhang, <b><font color="#002366">Shigang Li</font></b>, Ke Xu, Xiaoyan Liu, and Xuebin Chi. Intra-hour Photovoltaic Generation Forecasting based on Multi-source Data and Deep Learning Methods. IEEE Transactions on Sustainable Energy, 2021.
</span>
</li>

<li>
<span><b><font color="#002366">[PTRSA’2021]</font></b> &nbsp; Peter Grönquist, Chengyuan Yao, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. Deep Learning for Post-Processing Ensemble Weather Forecasts. Philosophical Transactions of the Royal Society A. [<a href= "https://arxiv.org/pdf/2005.08748.pdf">Paper</a>][<a href= "https://github.com/spcl/deep-weather">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2021]</font></b> &nbsp; Daning Cheng#, <b><font color="#002366">Shigang Li#</font></b>, Hanping Zhang, Fen Xia, and Yunquan Zhang. Why Dataset Properties Bound the Scalability of Parallel Machine Learning Training Algorithms. IEEE Transactions on Parallel and Distributed Systems (2021). [<a href= "https://ieeexplore.ieee.org/abstract/document/9316159">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2021]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Tal Ben-Nun, Dan Alistarh, Salvatore Di Girolamo, Nikoli Dryden, and Torsten Hoefler. Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging. IEEE Transactions on Parallel and Distributed Systems. [<a href= "https://ieeexplore.ieee.org/abstract/document/9271898">Paper</a>][<a href= "https://github.com/Shigangli/WAGMA-SGD">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[JPDC'2020]</font></b> &nbsp; Daning Cheng, <b><font color="#002366">Shigang Li*</font></b>, Yunquan Zhang. WP-SGD: Weighted parallel SGD for distributed unbalanced-workload training system. Journal of Parallel and Distributed Computing 145 (2020): 202-216. [<a href= "./files/jpdc2020.pdf">Paper</a>]
</span>
</li>

<li> <span><b><font color="#002366">[PPoPP'2020]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Tal Ben-Nun, Salvatore Di Girolamo, Dan Alistarh, and Torsten Hoefler. Taming unbalanced training workloads in deep learning with partial collective operations. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pp. 45-61. 2020. (Acceptance rate: 23%, 28/121; <font color="#FF0000">Best Paper Nomination</font>, 5/28)
[<a href= "https://dl.acm.org/doi/10.1145/3332466.3374528">Paper</a>][<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Talk</a>][<a href= "https://github.com/Shigangli/eager-SGD">Code</a>]
</span></li>

<li>
<span><b><font color="#002366">[JAMES'2020]</font></b> &nbsp; He Zhang, Minghua Zhang, ..., <b><font color="#002366">Shigang Li</font></b>, et al. CAS‐ESM 2: Description and climate simulation performance of the Chinese Academy of Sciences (CAS) Earth System Model (ESM) Version 2.  Journal of Advances in Modeling Earth Systems (2020): e2020MS002210.
</span>
</li>

<li>
<span><b><font color="#002366">[IPDPS'2020]</font></b> &nbsp; Hang Cao, Liang Yuan, He Zhang, Baodong Wu, <b><font color="#002366">Shigang Li</font></b>, Pengqi Lu, Yunquan Zhang, Yongjun Xu, and Minghua Zhang. A Highly Efficient Dynamical Core of Atmospheric General Circulation Model based on Leap-Format. In 2020 IEEE International Parallel and Distributed Processing Symposium, pp. 95-104. IEEE, 2020. [<a href= "./files/ipdps2020.pdf">Paper</a>]
</li>


<li>
<span><b><font color="#002366">[SC'2019]</font></b> &nbsp; Kun Li, Honghui Shang, Yunquan Zhang, <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, Dong Wang, Libo Zhang, Fang Li, Dexun Chen, and Zhiqiang Wei. OpenKMC: a KMC design for hundred-billion-atom simulation using millions of cores on Sunway Taihulight. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, p. 68. ACM, 2019. (Acceptance rate: 22.7%, 78/344)
</span>
</li>

<li>
<span><b><font color="#002366">[ICTAI'2019]</font></b> &nbsp; Daning Cheng, Hanping Zhang, Fen Xia, <b><font color="#002366">Shigang Li</font></b>, and Yunquan Zhang. Using Gradient based multikernel Gaussian Process and Meta-acquisition function to Accelerate SMBO. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pp. 440-447. IEEE, 2019.
</span>
</li>

<li>
<span><b><font color="#002366">[JSUPERCOMPUT'2019]</font></b> &nbsp; Kun Li, <b><font color="#002366">Shigang Li*</font></b>, Shan Huang, Yifeng Chen, and Yunquan Zhang. FastNBL: fast neighbor lists establishment for molecular dynamics simulation based on bitwise operations. The Journal of Supercomputing (2019): 1-20.
</span>
</li>

<li>
<span><b><font color="#002366">[ISPA'2019]</font></b> &nbsp; Kun Li, <b><font color="#002366">Shigang Li</font></b>, Bei Wang, Yifeng Chen, and Yunquan Zhang. swMD: Performance Optimizations for Molecular Dynamics Simulation on Sunway Taihulight. In 2019 IEEE International Symposium on Parallel & Distributed Processing with Applications, pp. 511-518. IEEE, 2019.
</span>
</li>

<!--
<li>
<span><b><font color="#002366">[ICPADS'2018]</font></b> Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Hang Cao, Yunquan Zhang, He Zhang, Junmin Xiao, and Minghua Zhang. <font color="#0000A0">AGCM3D: A Highly Scalable Finite-Difference Dynamical Core of Atmospheric General Circulation Model Based on 3D Decomposition</font>. In 2018 IEEE 24th International Conference on Parallel and Distributed Systems, pp. 355-364. IEEE, 2018 (Corresponding author).
</span>
</li>
-->

<li>
<span><b><font color="#002366">[TPDS'2018]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Torsten Hoefler. Cache-oblivious MPI all-to-all communications based on Morton order. IEEE Transactions on Parallel and Distributed Systems 29, no. 3 (2018): 542-555. [<a href= "https://ieeexplore.ieee.org/abstract/document/8091010">Paper</a>][<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Talk</a>][<a href= "https://github.com/Shigangli/COMPI">Code</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[ICPP'2018]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, Yunquan Zhang, Xianmeng Wang, Jianjiang Li, Changjun Hu, Jue Wang, Yangde Feng, and Ningming Nie. Massively scaling the metal microscopic damage simulation on sunway taihulight supercomputer. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1-11. 2018. [<a href= "https://dl.acm.org/doi/abs/10.1145/3225058.3225064">Paper</a>][<a href= "./files/ICPP-2018-slides.pdf">Slides</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[ICPP'2018]</font></b> &nbsp; Junmin Xiao, <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, He Zhang, Kun Li, Erlin Yao, Yunquan Zhang, and Guangming Tan. Communication-avoiding for dynamical core of atmospheric general circulation model. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1-10. 2018.
</span>
</li>

<li>
<span><b><font color="#002366">[JPDC'2018]</font></b> &nbsp; Zhihao Li, Haipeng Jia, Yunquan Zhang, Shice Liu, <b><font color="#002366">Shigang Li</font></b>, Xiao Wang, and Hao Zhang. Efficient parallel optimizations of a high-performance SIFT on GPUs. Journal of Parallel and Distributed Computing 124 (2019): 78-91.
</span>
</li>

<li>
<span><b><font color="#002366">[ICPADS'2018]</font></b> &nbsp; Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Hang Cao, Yunquan Zhang, He Zhang, Junmin Xiao, and Minghua Zhang. AGCM3D: A Highly Scalable Finite-Difference Dynamical Core of Atmospheric General Circulation Model Based on 3D Decomposition. In 2018 IEEE 24th International Conference on Parallel and Distributed Systems, pp. 355-364. IEEE, 2018. (Corresponding Author) [<a href= "./files/AGCM3D-icpads.pdf">Paper</a>][<a href= "./files/AGCM3D-slides.pdf">Slides</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[PPoPP'2017]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Torsten Hoefler. Cache-oblivious MPI all-to-all communications on many-core architectures. Poster, ACM SIGPLAN Notices 52, no. 8 (2017): 445-446. [<a href= "https://dl.acm.org/doi/10.1145/3155284.3019025">Paper</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[CPC'2017]</font></b> &nbsp; Changjun Hu, Xianmeng Wang, Jianjiang Li, Xinfu He, <b><font color="#002366">Shigang Li</font></b>, Yangde Feng, Shaofeng Yang, and He Bai. Kernel optimization for short-range molecular dynamics. Computer Physics Communications 211 (2017): 31-40.
</li>


<li>
<span><b><font color="#002366">[CPC'2017]</font></b> &nbsp; Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Yunquan Zhang, and Ningming Nie. Hybrid-optimization strategy for the communication of large-scale Kinetic Monte Carlo simulation. Computer Physics Communications 211 (2017): 113-123. (Corresponding Author)
</li>


<li>
<span><b><font color="#002366">[TACO'2016]</font></b> &nbsp; Yunquan Zhang, <b><font color="#002366">Shigang Li*</font></b>, Shengen Yan*, and Huiyang Zhou. A cross-platform spmv framework on many-core architectures. ACM Transactions on Architecture and Code Optimization (TACO) 13, no. 4 (2016): 1-25. (Corresponding Author) [<a href= "https://dl.acm.org/doi/10.1145/2994148">Paper</a>][<a href= "https://github.com/Shigangli/SpMV-on-Many-Core">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[PIEEE'2016]</font></b> &nbsp; Yunquan Zhang, Ting Cao, <b><font color="#002366">Shigang Li</font></b>, Xinhui Tian, Liang Yuan, Haipeng Jia, and Athanasios V. Vasilakos. Parallel processing systems for big data: a survey. Proceedings of the IEEE 104, no. 11 (2016): 2114-2136.
</li>

<li>
<span><b><font color="#002366">[SCI CHINA INFORM SCI'2015]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, ChangJun Hu, JunChao Zhang, and YunQuan Zhang. Automatic tuning of sparse matrix-vector multiplication on multicore clusters. Science China Information Sciences 58, no. 9 (2015): 1-14.
</li>

<li>
<span><b><font color="#002366">[HPCC'2015]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, Chunyang Xiang, and Lei Shi. Fast convolution operations on many-core architectures. In 2015 IEEE 17th International Conference on High Performance Computing and Communications, pp. 316-323. IEEE, 2015.[<a href= "https://ieeexplore.ieee.org/document/7336182">Paper</a>][<a href= "./files/HPCC15-slides.pdf">Slides</a>]
</li>

<li>
<span><b><font color="#002366">[CCGrid'2015]</font></b> &nbsp; Xiaomin Zhu, Junchao Zhang, Kazutomo Yoshii, <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Pavan Balaji. Analyzing MPI-3.0 process-level shared memory: A case study with stencil computations. In 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, Workshop, pp. 1099-1106. IEEE, 2015.
</li>

<li>
<span><b><font color="#002366">[CLUSTER COMPUT'2014]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler, Chungjin Hu, and Marc Snir. Improved MPI collectives for MPI processes in shared address spaces. Cluster computing 17, no. 4 (2014): 1139-1155.
</li>

<li>
<span><b><font color="#002366">[HPDC'2013]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler, and Marc Snir. NUMA-aware shared-memory collective communication for MPI. In Proceedings of the 22nd international symposium on High-performance parallel and distributed computing, pp. 85-96. 2013. (Acceptance rate: 15%, 20/131; <font color="#FF0000">Best Paper Nomination</font>, 3/20) [<a href= "https://dl.acm.org/doi/10.1145/2493123.2462903">Paper</a>]
</li>

<li>
<span><b><font color="#002366">[PDP'2013]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Jingyuan Hu, Xin Cheng, and Chongchong Zhao. Asynchronous work stealing on distributed memory systems. In 2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, pp. 198-202. IEEE, 2013.
</li>

<li>
<span><b><font color="#002366">[ICA3PP'2011]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Shucai Yao, Haohu He, Lili Sun, Yi Chen, and Yunfeng Peng. Extending synchronization constructs in openMP to exploit pipeline parallelism on heterogeneous multi-core. In International Conference on Algorithms and Architectures for Parallel Processing, Workshop, pp. 54-63. Springer, Berlin, Heidelberg, 2011.
</li>

<!--
<li>
<span><b><font color="#002366">[ICCS'2011]</font></b> &nbsp; Yunfeng Peng, Changjun Hu, Chongchong Zhao, <b><font color="#002366">Shigang Li</font></b>, and Shucai Yao. Management of Non-functional Attributes of Parallel Components. Procedia Computer Science 4 (2011): 461-470.
</li>

<li>
<span><b><font color="#002366">[ICA3PP'2010]</font></b> &nbsp; Qian Cao, Changjun Hu, Haohu He, Xiang Huang, and <b><font color="#002366">Shigang Li</font></b>. Support for OpenMP tasks on cell architecture. In International Conference on Algorithms and Architectures for Parallel Processing, Workshop, pp. 308-317. Springer, Berlin, Heidelberg, 2010.
</li>
-->
</font>
</ul>


<A NAME="Projects"><h2>Projects</h2></A>
<font size="4"> 
<ul>
<li> Project Leader &#151; MPI Model Extension and Performance Optimization for Many-Core Clusters, National Natural Science Foundation of China</li>
<li> Project Leader &#151; MPI Communication Optimization for Irregular Parallel Algorithms, State Key Laboratory of Computer Architecture Foundation </li>
<li> Technical Principal &#151; High Performance Deep Learning Library Development on CPU and GPU Architectures, IT Company Foundation </li>
<li> Technical Principal &#151; Large-Scale Deep Learning Training System on Heterogeneous Parallel Machines, IT Company Foundation </li>
</ul>
</font>

<A NAME="Teaching"><h2>Teaching</h2></A>
<font size="4"> 
<ul>
<li>Teaching Assistant &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2022-pp/">Parallel Programming, Spring 2022</a></li>
<li>Teaching Assistant &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2021-pp/">Parallel Programming, Spring 2021</a></li>
<li>Teaching Assistant &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2019-pp/">Parallel Programming, Spring 2019</a></li>
<!--
<li>Teaching Assistant &#151; ETH Zurich, <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2019W&ansicht=KATALOGDATEN&lerneinheitId=132388&lang=en">Numerical Methods for CSE, Autumn 2019</a>, C++/Eigen programming </li>
-->
<li>Teaching Assistant &#151; ETH Zurich, <a href="https://metaphor.ethz.ch/x/2018/hs/401-0663-00L/">Numerical Methods for CSE, Autumn 2018</a>, C++/Eigen programming </li>
</ul>
</font>

<A NAME="Talks"><h2>Talks</h2></A>
<font size="4"> 
<ul>
<li> <a href= "https://ppopp22.sigplan.org/">PPoPP'22</a> &#151; April 4, 2022 &#151; Near-Optimal Sparse Allreduce for Distributed Deep Learning. [<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Video</a>]</li>
<li> <a href= "https://sc21.supercomputing.org/program/papers/">SC'21</a> &#151; Nov. 16, 2021 &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145#sec-supp">Video</a>]</li>
<li> <a href= "https://www.emc2-ai.org/virtual-21">ECM2'21</a> &#151; Aug. 28, 2021 &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://www.youtube.com/watch?v=0ydk8u8ipmM">Video</a>]</li>
<li> <a href= "https://ppopp20.sigplan.org/">PPoPP'20</a> &#151; Feb. 24, 2020 &#151; Taming unbalanced training workloads in deep learning with partial collective operations. [<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Video</a>]</li>
<li> <a href= "https://sites.google.com/a/lbl.gov/padal-workshop/previous-padals/padal16/schedule">PADAL'16</a> &#151; Oct. 26, 2016 &#151; Cache-Oblivious MPI All-to-All Collectives. [<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Video</a>]</li>
</ul>
</font>

<A NAME="Services"><h2>Services</h2></A>
<font size="4"> 
<ul>
<li>Publicity Chair (Europe), PPoPP 2023</li>
<li>Publications Chair, IISWC 2020</li>
<li>Workshop Co-Chair, ICS 2018</li>
<li>Program Committee Member, SC 2022, 2021</li>
<li>Program Committee Member, PPoPP 2022</li>
<li>Program Committee Member, IEEE Cluster 2022, 2021</li>
<li>Program Committee Member, IPDPS 2021, 2018, 2017</li>
<li>Program Committee Member, ICPP 2022, 2017</li>
<li>Program Committee Member, ICPADS 2022, 2018</li>
<li>Program Committee Member, HPC Asia 2021, 2020, 2019, 2018</li>
<li>Program Committee Member, HPC China 2021, 2019, 2018, 2017, 2016</li>
<li>Program Committee Member, SBAC-PAD 2022, 2020, 2016 (ERC)</li>
<li>Program Committee Member, PMAM 2022, 2021</li>
<li>Program Committee Member, HP3C 2020, 2019, 2018</li>
<li>Program Committee Member, INFOCOMP 2022, 2021, 2020</li>
<br>
<li>Associate editor of Cluster Computing (CLUS) - Springer</li>
<li>Reviewer of IEEE Transactions on Parallel and Distributed Systems (TPDS)</li>
<li>Reviewer of IEEE Transactions on Services Computing (TSC)</li>
<li>Reviewer of IEEE Transactions on Big Data (TBD)</li>
<li>Reviewer of IEEE Transactions on Network Science and Engineering</li>
<li>Reviewer of IEEE Transactions on Circuits and Systems II: Express Briefs</li>
<li>Reviewer of Journal of Parallel and Distributed Computing (JPDC) - Elsevier</li>
<li>Reviewer of Journal of Supercomputing - Springer</li>
<li>Reviewer of Concurrency and Computation: Practice and Experience</li>
<li>Reviewer of Mobile Networks and Applications – Springer</li>
<li>Program Committee Member, IEEE TPDS Special Section on Parallel and Distributed Computing Techniques for AI, ML, and DL, 2020</li>
</ul>
</font>




<br />
 
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ymalyc7n8v&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> 

</body>
</html>
